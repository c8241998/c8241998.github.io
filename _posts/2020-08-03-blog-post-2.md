---
title: 'ActivityNet Challenge 2019 冠军模型BMN算法全解析'
date: 2020-05-27
permalink: /posts/2020/05/blog-post-1/
tags:
  - temporal action detection
  - computer vision
  - deep learning
---

随着互联网世界视频数量的急剧增长，视频内容分析技术得到了越来越广泛关注。视频动作定位是视频内容分析领域里的一个重要任务，它的目标是在一段未经修剪的视频中定位出动作类别及其对应的时序边界。通常视频动作定位任务可以分为两个阶段：时序提名生成和动作分类。本文所介绍的算法BMN为百度自研，为时序提名生成任务提供了一个高效的解决方案，是ActivityNet2019大赛中的task1和task2的冠军方案（task1为时序提名生成，task2为视频动作定位）。

模型概览
======

<figure class="half" align=center>
    <img src="/images/1/1.jpg" width="40%" alt="BMN算法概览">
    <img src="/images/1/2.png" width="40%" alt="网络配置图">
</figure>

作为BSN系列的改进方案，BMN算法继承了BSN的整体思路：先定位动作的边界，再将边界节点组合为时序提名，对于候选的时序提名，基于置信度分数进行筛选，得到最终的动作定位集合。  
BMN模型的处理流程如下：首先使用双流网络（光流+RGB）对输入视频进行预处理，得到编码特征。然后将编码特征输入Base Module，使用两层时序卷积提取特征，其后分TEM和PEM两个分支进行处理，TEM模块通过两层时序卷积处理得到视频每个位置为开始边界的概率序列和为结束边界的概率序列，PEM模块处理得到BM配置信度图（该过程将在下文中详细介绍），使用该置信度图可以轻松得到任一候选提名的置信度分数。  
在后处理阶段，边界概率两个序列中的极大值点及超过最大值一半的点所在视频位置被视为候选开始边界或候选结束边界。通过对两种边界的两两组合，我们可以得到一系列的候选动作提名，再通过在置信度图中检索分数，我们可以得到一个拥有置信度分数的候选提名集合。最后，算法采用了Soft-NMS方法来去除了冗余结果。  


边界匹配机制
======

BMN提出了一种新的时序提名置信度评估机制——边界匹配机制，作用于PEM模块。边界匹配机制的目的是为生成的候选提名提出可靠的置信度分数，下面笔者将从以下三个方面对该机制进行讲解。

BM置信度图
------

首先我们将一个时序动作提名$\phi$表示为开始边界为$t_S$，结束边界为 $t_e$ 的匹配对。如下图所示，边界匹配机制的目的是创建由有不同开始边界和长度的边界匹配对组成的二维BM置信度图$M_C$。我们用$M_C(i,j)$表示提名$\phi(i,j)$的置信度分数，其中$\phi(i,j)$的开始边界$t_s=t_j$，持续时长$d=t_i$，因此其结束边界$t_e=t_j+t_i$。因此，我们可以使用BM置信度图来为候选的时序提名获取置信度分数。

<img src="/images/1/3.png" width="80%">

BM置信度图中，分布在同一行的提名拥有相同的持续长度，分布在同一列的提名拥有相同的开始边界，分布在副对角线的提名拥有相同的结束边界。右下角部分提名因为结束边界超出视频范围，因此归为无意义区域。

BM layer
------

如何从视频特征序列提取出BM置信度图是一个关键的问题，该算法引入一个BM layer来完成这个任务。视频时序序列特征为$S_F \in R^{C\times T}$，通过BM layer我们可以得到BM特征图$M_F\in R^{C\times N\times D\times T}$，随后经过一系列卷积层得到BM置信度图$M_C\in R^{D\times T}$，其中D为预定义的时序动作提名最大长度。
BM layer的作用是：对于提名$\phi(i,j)$，我们将在$S_F$中均匀采样N个点以获得拥有丰富上下文信息的提名特征$m_{i,j}^f\in R^{C\times N}$。随后我们可以通过对所有提名同时采用这种采样策略来获取边界匹配特征图$M_F$。

<img src="/images/1/4.png" width="80%">

然而采样过程中依然还有两个难点待解决：（1）如何从非整数点采样特征？（2）如何对所有提名同时采样？如上图所示，我们采用一个采样掩码矩阵$W\in R^{N\times T\times D\times T}$与时序特征序列矩阵$S_F\in R^{C\times T}$在时序维度相乘来同时解决这两个问题。首先，对于每个提名$\phi(i,j)$，我们通过上述增广采样方法构建一个权重项$w_{i,j}\in R^{N\times T}$。对于一个非整数的采样点$t_n$，我们定义它的采样掩码$w_{i,j,n}\in R^T$为：  

$$
f(x)=\left\{\begin{aligned}
1-\operatorname{dec}\left(t_{n}\right), & \text { if } t=\text { floor }\left(t_{n}\right) \\
\operatorname{dec}\left(t_{n}\right), & \text { if } t=\text { floor }\left(t_{n}\right)+1 \\
0, & \text { if } t=\text { others }
\end{aligned}\right.
$$  

其中dec和floor分别指取小数部分函数和取整数部分函数。进而，我们可以获得权重项$w_{i,j}\in R^{N\times T}$。随后，通过在时序维度矩阵相乘$S_F$和$w_{i,j}$，我们可以得到提名特征：  

$$
m_{i, j}^{f}[c, n]=\sum_{t=1}^{T} S_{f}[c, t] \cdot w_{i, j}[n, t]
$$  

通过将单个提名的权重项$w_{i,j}\in R^{N\times T}$扩展到BM置信度图中所有提名，我们可以得到权重矩阵$W\in R^{N\times T\times D\times T}$，最后使用矩阵相乘便可得到BM特征度图$M_F\in R^{C\times N\times D\times T}$。因为采样掩码矩阵$W$对任何视频来说都是相同的，因此它可以被提前计算，进而在推理过程中BM player的计算速度将会非常快。

BM标签
------

在训练时，我们将BM标签图记为$G_C\in R^{D\times T}$，它与BM置信度图$M_C$拥有相同的维度和形状，其中$g_{i,j}^c\in[0,1]$表示候选提名$\phi(i,j)$与所有真实提名的最大交并比。总体来说，在边界匹配机制中，我们使用BM layer从时序特征序列$S_F$中高效地生成BM特征图$M_F$，随后使用一系列卷积来生成BM置信度图$M_C$，整个过程将使用BM标签图$G_C$来进行有监督的训练。


PaddlePaddle复现
======

[**code link**](https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/video/models/bmn)


总结
======

最终使用飞桨对BMN复现在ActivityNet1.3数据集的验证集上评估精度如下:  

|      AR@1    |      AR@5    |     AR@10    |     AR@100    |       AUC     |
|:------------:|:------------:|:------------:|:-------------:|:-------------:|
|     33.06    |     49.13    |     56.27    |      75.32    |     67.19%    |  

BMN算法是时序动作检测领域的经典之作，它的主要贡献是通过引入一种全新的时序提名评估机制以及高效的特征采样方式，来提升了前作BSN方法的性能和效率。但是因其网络较为简单，因此其网络设计和优化方面依然存在着提高空间。AAAI2020的一篇工作《Fast Learning of Temporal Action Proposal via Dense Boundary Generator》通过优化损失函数、改进网络设计的方式进一步提高了该系列工作的性能指标，有兴趣的同学可以自行阅读。
